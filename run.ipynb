{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basics import *\n",
    "from implementations import *\n",
    "from processing import *\n",
    "from cleaning import *\n",
    "from metrics import *\n",
    "from helper import *\n",
    "from definitions import ROOT_DIR\n",
    "\n",
    "import implementations as imp\n",
    "import helper as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(ROOT_DIR, 'dataset_to_release')\n",
    "x_tr, x_te, y_tr, tr_ids, te_ids = load_csv_data(dataset_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 321)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = True #for the sake of time, if set to False will use premade list of best parameters already computed. Those same parameters can be found by keeping cv to True. \n",
    "split_ratio = 0.8 #80% of the dataset will be used to train the model, the rest will be used to evaluate performance\n",
    "nan_threshold = 0.8 #to filter our features with more nan than the threshold\n",
    "max_unique_values = 50 \n",
    "remove_const = False\n",
    "const_thresholds = [0.0000001, 0.000001] #to filter constant continuous , when filtering out features w too many categories\n",
    "n_components = False\n",
    "PCA = False #if set to True will run PCA on continuous features and keep principal components until 99% of variance is explained\n",
    "pca_thresholds = 99 #in % explained variance\n",
    "correlation_threshold = 0.9 #correlation threshold\n",
    "\n",
    "models = ['gradient descent', 'stochastic gradient descent', 'least squares', 'ridge regression',  'logistic regression', 'reg logistic regression']\n",
    "mapping_threshold = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing + cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning - data is now (328135, 559)\n"
     ]
    }
   ],
   "source": [
    "#split the data\n",
    "\n",
    "clean_train = clean(x_tr, nan_threshold, \n",
    "                    remove_const, const_thresholds, \n",
    "                    PCA, n_components, pca_thresholds, \n",
    "                    max_unique_values, correlation_threshold)\n",
    "\n",
    "clean_x_tr, clean_x_te, y_tr_set, y_te_set = split_data(clean_train, y_tr, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 559)\n"
     ]
    }
   ],
   "source": [
    "print(clean_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUNE HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculates the gradient of the unregularized loss and uses it in gradient descent to approximate optimal weights\n",
    "    y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): shape = (N,) contains the data we want to predict\n",
    "        tx (np.ndarray): shape = (N,2) contains the features used to predict\n",
    "        initial_w (np.ndarray): shape = (2,) the initial weight pair that will get updated with gradient\n",
    "        max_iters (int): maximum number of steps\n",
    "        gamma (float): learning rate\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray : shape = (2,) optimal weights\n",
    "        float : mean squared erros\n",
    "    \"\"\"\n",
    "    w = initial_w # initiate w_{t}\n",
    "    for n_iter in range(max_iters):\n",
    "        e = y - np.dot(tx,w)\n",
    "        gradient = -tx.T.dot(e) / len(e)\n",
    "        w = w - gamma * gradient # w_{t+1} = w_{t} - gamma * \\/L(w_{t})\n",
    "    e = y - np.dot(tx,w)\n",
    "    return w, compute_mse(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses_for_hyperparameters(model, y, tx, k_fold, max_iters=0, lambdas = ['Nan'], gammas = ['Nan'], seed = 1):\n",
    "    \"\"\"Process cross-validation with the chosen model \n",
    "        Calculate the test and train errors for every hyperparameters \n",
    "\n",
    "    Args:\n",
    "        model (string): name of the regression technique chosen\n",
    "        y (np.ndarray): shape = (N,) contains the data we want to predict\n",
    "        tx (np.ndarray): shape = (N,D) contains the features used to predict\n",
    "        initial_w (np.ndarray): shape = (D,) the initial weight pair that will get updated with gradient\n",
    "        max_iters (int): maximum number of steps\n",
    "        lambdas (np.ndarray): hyperparameter for the penalized loss for regularized regression\n",
    "        gammas (np.ndarray): hyperparameter for GD and SGD implementation\n",
    "        k_fold (int): K in K-fold, i.e. the fold num\n",
    "        seed (int):  the random seed\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray : shape(N,5) train and test errors for each hyperparameters of the chosen model\n",
    "    \"\"\"\n",
    "\n",
    "    results = np.array([\"model\", \"lambda\", \"gamma\", \"train error\", \"test error\"])\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    for lambda_ in lambdas:\n",
    "        for gamma in gammas:\n",
    "            losses_tr = []\n",
    "            losses_te = []\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te = cv_loss(model, y, tx, k_indices, k, lambda_, max_iters, gamma)\n",
    "                losses_tr.append(loss_tr)\n",
    "                print('losses tr' , losses_tr)\n",
    "                losses_te.append(loss_te)\n",
    "            loss_tr = np.mean(losses_tr)\n",
    "            loss_te = np.mean(losses_te)\n",
    "            res =  np.array([model, lambda_, gamma, loss_tr, loss_te])\n",
    "            print(res)\n",
    "            results = np.append(results, res)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def cv_loss(model, y, x, k_indices, k, lambda_, max_iters, gamma): \n",
    "    \"\"\"to complete ????\n",
    "\n",
    "    Args:\n",
    "        model:      str, ['gradient descent', 'stochastic gradient descent', 'ridge regression', 'logistic regression', 'reg logistic regression]\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        initial_w:  scalar, default to 0, needed for all models but ridge regression\n",
    "        max_iters:  scalar, default to 0, needed for all models but ridge regression\n",
    "        gamma:      learning rate, default to 0, needed for all models but ridge regression\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    # get k'th subgroup in test, others in train: \n",
    "    test = k_indices[k]\n",
    "    train = np.delete(k_indices, k, axis=0)\n",
    "    \n",
    "    y_te = np.array([y[i] for i in test])\n",
    "    x_te = np.array([x[i] for i in test])\n",
    "    \n",
    "    y_tr = np.array([y[i] for i in train.flatten()])\n",
    "    x_tr = np.array([x[i] for i in train.flatten()])\n",
    "\n",
    "    initial_w = np.zeros(x.shape[1])\n",
    "    if model == 'gradient descent':\n",
    "        w, _ = mean_squared_error_gd(y_tr,x_tr, initial_w, max_iters, gamma)\n",
    "        \n",
    "    elif model == 'stochastic gradient descent':\n",
    "        w, _ = imp.mean_squared_error_sgd(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "        \n",
    "    elif model == 'ridge regression':\n",
    "        w, _ = imp.ridge_regression(y_tr, x_tr, lambda_)\n",
    "        \n",
    "    elif model == 'logistic regression':\n",
    "        w, _ = imp.logistic_regression(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "        \n",
    "    elif model == 'reg logistic regression':\n",
    "        w, _ = imp.reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    # calculate the loss for train and test data: TODO\n",
    "    te_err = y_te - np.dot(x_te, w)\n",
    "    loss_te = np.sqrt(2*hp.compute_mse(te_err))\n",
    "    print('w is : ', w)\n",
    "    tr_err = y_tr - np.dot(x_tr, w)\n",
    "    loss_tr = np.sqrt(2*hp.compute_mse(tr_err))\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(hyperparameter_losses):\n",
    "    \"\"\"Calculate the best hyperparameters based on the test errors computed previously\n",
    "\n",
    "    Args:\n",
    "        np.ndarray : shape(N,5) train and test errors for each hyperparameters of the chosen model\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray : shape (1,5) train and test errors for the best hyperparameters of the chosen model\n",
    "    \"\"\"\n",
    "\n",
    "    min_index = np.argmin(hyperparameter_losses[1:,-1])\n",
    "    return hyperparameter_losses[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams = [lambda_, gamma, ]\n",
    "best_params = {'gradient descent' : [0.01, 0.7], \n",
    "               'stochastic gradient descent': [0.01, 0.7],\n",
    "               'least squares': [0.01, 0.7],\n",
    "               'ridge regression': [0.01, 0.7],\n",
    "               'logistic regression': [0.01, 0.7],\n",
    "               'reg logistic regression': [0.01, 0.7]}#complete w our results as ['model', lambda_, gamma]\n",
    "\n",
    "k_fold = 4\n",
    "max_iters = 500000\n",
    "lambdas = np.linspace(0, 1, 10)\n",
    "gammas = np.linspace(0.5,1.5,5)\n",
    "\n",
    "if cv:\n",
    "    best_params = {}\n",
    "    for model in models:\n",
    "        losses = compute_losses_for_hyperparameters(model, y_tr_set, clean_x_tr, k_fold, max_iters, lambdas, gammas)\n",
    "        print(losses.shape)\n",
    "        best_params['model'] = [find_best_hyperparameters(losses)[1:3]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in best_params.items():\n",
    "    print(f'{key} - {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'gradient descent' : [0.01, 0.7], \n",
    "               'stochastic gradient descent': [0.01, 0.7],\n",
    "               'least squares': [0.01, 0.7],\n",
    "               'ridge regression': [0.01, 0.7],\n",
    "               'logistic regression': [0.01, 0.7],\n",
    "               'reg logistic regression': [0.01, 0.7]}#complete w our results as ['model', lambda_, gamma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing features\n",
    "absurd_tr = clean_x_tr[:, :15]\n",
    "absurd_te = clean_x_te[:, :15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on least squares\n",
      "\n",
      "---------- {LEAST SQUARES} ----------\n",
      "train rmse = 0.9494482341806864 - test rmse = 0.9497750431223022\n",
      "=========== TRAIN metrics ===========\n",
      "Accuracy: 0.6336416413975955 - F1 score 0.7650566231983528\n",
      "Specificity: 0.620833960434121 - Sensitivity: 0.7650566231983528\n",
      "Precision: 0.7650566231983528\n",
      "=========== TEST metrics ===========\n",
      "Accuracy: 0.6348301766041415 - F1 score 0.7647889811054212\n",
      "Specificity: 0.6225568674538056 - Sensitivity: 0.7647889811054212\n",
      "Precision: 0.7647889811054212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = 'least squares'\n",
    "lambda_ = best_params[model][0]\n",
    "gamma = best_params[model][1]\n",
    "print(f'working on least squares')\n",
    "w, loss = least_squares(y_tr_set, absurd_tr)\n",
    "train_predictions = np.where(np.dot(absurd_tr,w) < mapping_threshold, -1, 1)\n",
    "test_predictions = np.where(np.dot(absurd_te, w) < mapping_threshold, -1, 1)\n",
    "tr_err = y_tr_set - np.dot(absurd_tr, w)\n",
    "te_err = y_te_set - np.dot(absurd_te, w)\n",
    "train_rmse = np.sqrt(2*compute_mse(tr_err))\n",
    "test_rmse = np.sqrt(2*compute_mse(te_err))\n",
    "print('')\n",
    "print('---------- {LEAST SQUARES} ----------')\n",
    "print(f'train rmse = {train_rmse} - test rmse = {test_rmse}')\n",
    "train_metrics = calculate_metrics(y_tr_set, train_predictions)\n",
    "test_metrics = calculate_metrics(y_te_set, test_predictions)\n",
    "print('=========== TRAIN metrics ===========')\n",
    "prettyprint(train_metrics)\n",
    "print('=========== TEST metrics ===========')\n",
    "prettyprint(test_metrics)\n",
    "print('')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on gradient descent\n",
      "\n",
      "---------- GRADIENT DESCENT ----------\n",
      "train rmse = 0.9494482341806864 - test rmse = 0.9497750431223022\n",
      "=========== TRAIN metrics ===========\n",
      "Accuracy: 0.6336416413975955 - F1 score 0.7650566231983528\n",
      "Specificity: 0.620833960434121 - Sensitivity: 0.7650566231983528\n",
      "Precision: 0.7650566231983528\n",
      "=========== TEST metrics ===========\n",
      "Accuracy: 0.6348301766041415 - F1 score 0.7647889811054212\n",
      "Specificity: 0.6225568674538056 - Sensitivity: 0.7647889811054212\n",
      "Precision: 0.7647889811054212\n",
      "\n",
      "working on stochastic gradient descent\n",
      "\n",
      "---------- STOCHASTIC GRADIENT DESCENT ----------\n",
      "train rmse = 2.225869390814902e+82 - test rmse = 2.2320636419937186e+82\n",
      "=========== TRAIN metrics ===========\n",
      "Accuracy: 0.4141207125116187 - F1 score 0.24579615648593\n",
      "Specificity: 0.4305255940734795 - Sensitivity: 0.24579615648593\n",
      "Precision: 0.24579615648593\n",
      "=========== TEST metrics ===========\n",
      "Accuracy: 0.41428070763557684 - F1 score 0.24633586438283595\n",
      "Specificity: 0.4301414181842439 - Sensitivity: 0.24633586438283595\n",
      "Precision: 0.24633586438283595\n",
      "\n",
      "working on least squares\n",
      "\n",
      "---------- LEAST SQUARES ----------\n",
      "train rmse = 0.9494482341806864 - test rmse = 0.9497750431223022\n",
      "=========== TRAIN metrics ===========\n",
      "Accuracy: 0.6336416413975955 - F1 score 0.7650566231983528\n",
      "Specificity: 0.620833960434121 - Sensitivity: 0.7650566231983528\n",
      "Precision: 0.7650566231983528\n",
      "=========== TEST metrics ===========\n",
      "Accuracy: 0.6348301766041415 - F1 score 0.7647889811054212\n",
      "Specificity: 0.6225568674538056 - Sensitivity: 0.7647889811054212\n",
      "Precision: 0.7647889811054212\n",
      "\n",
      "working on ridge regression\n",
      "\n",
      "---------- RIDGE REGRESSION ----------\n",
      "train rmse = 0.9504364340383684 - test rmse = 0.9507607073207326\n",
      "=========== TRAIN metrics ===========\n",
      "Accuracy: 0.633912109345239 - F1 score 0.7648850377487988\n",
      "Specificity: 0.6211475108279403 - Sensitivity: 0.7648850377487989\n",
      "Precision: 0.7648850377487989\n",
      "=========== TEST metrics ===========\n",
      "Accuracy: 0.6353787313148551 - F1 score 0.7642592265583613\n",
      "Specificity: 0.623207257687946 - Sensitivity: 0.7642592265583613\n",
      "Precision: 0.7642592265583613\n",
      "\n",
      "working on logistic regression\n",
      "\n",
      "---------- LOGISTIC REGRESSION ----------\n",
      "train rmse = 9.265930123069536 - test rmse = 9.268440771211324\n",
      "=========== TRAIN metrics ===========\n",
      "Accuracy: 0.6393709905983818 - F1 score 0.7641986959505834\n",
      "Specificity: 0.627205304436529 - Sensitivity: 0.7641986959505834\n",
      "Precision: 0.7641986959505834\n",
      "=========== TEST metrics ===========\n",
      "Accuracy: 0.6404833376506621 - F1 score 0.7635528871622814\n",
      "Specificity: 0.6288606497231672 - Sensitivity: 0.7635528871622814\n",
      "Precision: 0.7635528871622814\n",
      "\n",
      "working on reg logistic regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivaberlenghi/Library/Mobile Documents/com~apple~CloudDocs/MA1/MACHINE LEARNING/ml-project-1-bytemeifyoucan/helper.py:136: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-t))\n",
      "/Users/vivaberlenghi/Library/Mobile Documents/com~apple~CloudDocs/MA1/MACHINE LEARNING/ml-project-1-bytemeifyoucan/implementations.py:151: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = - np.mean(y * np.log(s) + (1 - y) * np.log(1 - s))\n",
      "/Users/vivaberlenghi/opt/anaconda3/envs/mlcourse/lib/python3.11/site-packages/numpy/core/_methods.py:118: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- REG LOGISTIC REGRESSION ----------\n",
      "train rmse = 1532.8435654014916 - test rmse = 1533.771820052574\n",
      "=========== TRAIN metrics ===========\n",
      "Accuracy: 0.8166303503131334 - F1 score 0.540108098833219\n",
      "Specificity: 0.8435801602033479 - Sensitivity: 0.540108098833219\n",
      "Precision: 0.540108098833219\n",
      "=========== TEST metrics ===========\n",
      "Accuracy: 0.8153808645831746 - F1 score 0.5401730531520396\n",
      "Specificity: 0.8413714895604029 - Sensitivity: 0.5401730531520396\n",
      "Precision: 0.5401730531520396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(absurd_tr.shape[1])\n",
    "th = [0,0,0,0,0]\n",
    "\n",
    "for model in models:\n",
    "    lambda_ = best_params[model][0]\n",
    "    gamma = best_params[model][1]\n",
    "    print(f'working on {model}')\n",
    "    w, loss = train(model,y_tr_set, absurd_tr, initial_w, 500, gamma, lambda_)\n",
    "    train_predictions = np.where(np.dot(absurd_tr,w) < mapping_threshold, -1, 1)\n",
    "    test_predictions = np.where(np.dot(absurd_te, w) < mapping_threshold, -1, 1)\n",
    "    tr_err = y_tr_set - np.dot(absurd_tr, w)\n",
    "    te_err = y_te_set - np.dot(absurd_te, w)\n",
    "    train_rmse = np.sqrt(2*compute_mse(tr_err))\n",
    "    test_rmse = np.sqrt(2*compute_mse(te_err))\n",
    "    print('')\n",
    "    print(f'---------- {model.upper()} ----------')\n",
    "    print(f'train rmse = {train_rmse} - test rmse = {test_rmse}')\n",
    "    train_metrics = calculate_metrics(y_tr_set, train_predictions)\n",
    "    test_metrics = calculate_metrics(y_te_set, test_predictions)\n",
    "    print('=========== TRAIN metrics ===========')\n",
    "    prettyprint(train_metrics)\n",
    "    print('=========== TEST metrics ===========')\n",
    "    prettyprint(test_metrics)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(clean_x_tr.shape[1])\n",
    "for model in models:\n",
    "    lambda_ = best_params[model][0]\n",
    "    gamma = best_params[model][1]\n",
    "    print(f'working on {model}')\n",
    "    w, loss = train(model, y_tr_set, clean_x_tr, initial_w, max_iters, gamma, lambda_)\n",
    "    train_predictions = np.where(np.dot(clean_x_tr,w) < mapping_threshold, -1, 1)\n",
    "    test_predictions = np.where(np.dot(clean_x_te, w) < mapping_threshold, -1, 1)\n",
    "    train_err = y_tr_set - train_predictions\n",
    "    test_err = y_te_set - test_predictions\n",
    "    print('')\n",
    "    print(f'=========== {model} ===========')\n",
    "    print(f'train error = {train_err} - test error = {test_err}')\n",
    "    train_metrics = calculate_metrics(y_tr_set, train_predictions)\n",
    "    test_metrics = calculate_metrics(y_te_set, test_predictions)\n",
    "    print('=========== TRAIN metrics ===========')\n",
    "    prettyprint(train_metrics)\n",
    "    print('=========== TEST metrics ===========')\n",
    "    prettyprint(test_metrics)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat all steps for total train and make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning - data is now (437514, 515)\n"
     ]
    }
   ],
   "source": [
    "#cleaning\n",
    "#some parameters might need to be taken from hyperparameters\n",
    "limit = x_tr.shape[0]\n",
    "\n",
    "total_data = np.concatenate((x_tr, x_te), axis = 0)\n",
    "\n",
    "clean_total = clean(total_data, nan_threshold, \n",
    "                    remove_const, const_thresholds, \n",
    "                    PCA, n_components, pca_thresholds, \n",
    "                    max_unique_values, correlation_threshold)\n",
    "\n",
    "tot_clean_train = clean_total[:limit]\n",
    "\n",
    "tot_clean_test = clean_total[limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg logistic regression\n",
      "about to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivaberlenghi/Library/Mobile Documents/com~apple~CloudDocs/MA1/MACHINE LEARNING/ml-project-1-bytemeifyoucan/helper.py:136: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "#run on absurd train and predict + make submission prediction\n",
    "abs_tot_tr = tot_clean_train[:, :15]\n",
    "abs_tot_te = tot_clean_test[:,:15]\n",
    "\n",
    "model = 'reg logistic regression'\n",
    "lambda_ = best_params[model][0]\n",
    "gamma = best_params[model][1]\n",
    "initial_w = np.zeros(abs_tot_tr.shape[1])\n",
    "print(model)\n",
    "print('about to train')\n",
    "w, loss = train(model, y_tr, abs_tot_tr, initial_w, 50000, gamma, lambda_)\n",
    "print('mapping predictions')\n",
    "y_pred = np.where(np.dot(abs_tot_te,w) < mapping_threshold, -1, 1)\n",
    "print('about to create submission')\n",
    "create_csv_submission([i for i in range(len(y_tr), len(y_tr)+len(y_pred))], y_pred, model.replace(' ' , '_') + '_ABSURD_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109379\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent\n",
      "about to train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vivaberlenghi/Library/Mobile Documents/com~apple~CloudDocs/MA1/MACHINE LEARNING/ml-project-1-bytemeifyoucan/run.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vivaberlenghi/Library/Mobile%20Documents/com~apple~CloudDocs/MA1/MACHINE%20LEARNING/ml-project-1-bytemeifyoucan/run.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vivaberlenghi/Library/Mobile%20Documents/com~apple~CloudDocs/MA1/MACHINE%20LEARNING/ml-project-1-bytemeifyoucan/run.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mabout to train\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vivaberlenghi/Library/Mobile%20Documents/com~apple~CloudDocs/MA1/MACHINE%20LEARNING/ml-project-1-bytemeifyoucan/run.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m w, loss \u001b[39m=\u001b[39m train(model, y_tr, abs_tot_tr, initial_w, max_iters, gamma, lambda_)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vivaberlenghi/Library/Mobile%20Documents/com~apple~CloudDocs/MA1/MACHINE%20LEARNING/ml-project-1-bytemeifyoucan/run.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmapping predictions\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vivaberlenghi/Library/Mobile%20Documents/com~apple~CloudDocs/MA1/MACHINE%20LEARNING/ml-project-1-bytemeifyoucan/run.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39mdot(abs_tot_te,w) \u001b[39m<\u001b[39m mapping_threshold, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/MA1/MACHINE LEARNING/ml-project-1-bytemeifyoucan/basics.py:6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, y, x, initial_w, max_iters, gamma, lambda_, cost)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, y, x, initial_w \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, max_iters \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, gamma \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, lambda_ \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, cost \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgradient descent\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m         w, loss \u001b[39m=\u001b[39m mean_squared_error_gd(y,x, initial_w, max_iters, gamma)\n\u001b[1;32m      8\u001b[0m     \u001b[39melif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstochastic gradient descent\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      9\u001b[0m         w, loss \u001b[39m=\u001b[39m mean_squared_error_sgd(y, x, initial_w, max_iters, gamma)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/MA1/MACHINE LEARNING/ml-project-1-bytemeifyoucan/implementations.py:24\u001b[0m, in \u001b[0;36mmean_squared_error_gd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m n_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m     23\u001b[0m     e \u001b[39m=\u001b[39m y \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mdot(tx,w)\n\u001b[0;32m---> 24\u001b[0m     gradient \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtx\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(e) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(e)\n\u001b[1;32m     25\u001b[0m     new_w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m gamma \u001b[39m*\u001b[39m gradient \u001b[39m# w_{t+1} = w_{t} - gamma * \\/L(w_{t})\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     w \u001b[39m=\u001b[39m new_w \u001b[39m# update w_{t} with the value of w_{t+1} for the next iteration\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run on absurd train and predict + make submission prediction\n",
    "abs_tot_tr = tot_clean_train[:, :20]\n",
    "abs_tot_te = tot_clean_test[:,:20]\n",
    "\n",
    "for model in models:\n",
    "    lambda_ = best_params[model][0]\n",
    "    gamma = best_params[model][1]\n",
    "    initial_w = np.zeros(abs_tot_tr.shape[1])\n",
    "    print(model)\n",
    "    print('about to train')\n",
    "    w, loss = train(model, y_tr, abs_tot_tr, initial_w, max_iters, gamma, lambda_)\n",
    "    print('mapping predictions')\n",
    "    y_pred = np.where(np.dot(abs_tot_te,w) < mapping_threshold, -1, 1)\n",
    "    print('about to create submission')\n",
    "    create_csv_submission([i for i in range(len(y_pred))], y_pred, model.replace(' ' , '_') + '_ABSURD_submission')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on train and predict + submit prediction\n",
    "for model in models:\n",
    "    lambda_ = best_params[model][0]\n",
    "    gamma = best_params[model][1]\n",
    "    initial_w = np.zeros(y_tr)\n",
    "    w, loss = train(model, y_tr, tot_clean_train, initial_w, max_iters, gamma, lambda_)\n",
    "    y_pred = np.where(np.dot(tot_clean_test,w) < mapping_threshold, -1, 1)\n",
    "    create_csv_submission([i for i in range(len(y_pred))], y_pred, model.replace( , '_') + '_submission')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
