{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basics import *\n",
    "from implementations import *\n",
    "from processing import *\n",
    "from cleaning import *\n",
    "from metrics import *\n",
    "from helper import *\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(ROOT_DIR, 'dataset_to_release')\n",
    "x_tr, x_te, y_tr, tr_ids, te_ids = load_csv_data(dataset_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = True #for the sake of time, if set to False will use premade list of best parameters already computed. Those same parameters can be found by keeping cv to True. \n",
    "split_ratio = 0.8 #80% of the dataset will be used to train the model, the rest will be used to evaluate performance\n",
    "nan_threshold = 0.8 #to filter our features with more nan than the threshold\n",
    "max_unique_values = 50 \n",
    "remove_const = False\n",
    "const_thresholds = [0.0000001, 0.000001] #to filter constant continuous , when filtering out features w too many categories\n",
    "n_components = False\n",
    "PCA = False #if set to True will run PCA on continuous features and keep principal components until 99% of variance is explained\n",
    "pca_thresholds = 99 #in % explained variance\n",
    "correlation_threshold = 0.9 #correlation threshold\n",
    "\n",
    "models = ['gradient descent', 'stochastic gradient descent', 'least squares', 'ridge regression',  'logistic regression', 'reg logistic regression']\n",
    "mapping_threshold = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing + cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "x_tr_set, x_te_set, y_tr_set, y_te_set = split_data(x_tr, y_tr, split_ratio)\n",
    "\n",
    "clean_train = clean(x_tr_set, nan_threshold, \n",
    "                    remove_const, const_thresholds, \n",
    "                    PCA, n_components, pca_thresholds, \n",
    "                    max_unique_values, correlation_threshold)\n",
    "clean_test = clean(x_te_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUNE HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams = [lambda_, gamma, ]\n",
    "best_params = {}#complete w our results as ['model', lambda_, gamma]\n",
    "k_fold = 4\n",
    "max_iters = 500\n",
    "lambdas = np.linspace(0, 1, 10)\n",
    "gammas = np.linspace(0,2,5)\n",
    "\n",
    "if cv:\n",
    "    best_params = {}\n",
    "    for model in models:\n",
    "        losses = compute_losses_for_hyperparameters(model, y_tr, x_tr, k_fold,  np.zeros(len(y_tr)), max_iters, lambdas, gammas)\n",
    "        best_params['model'] = [find_best_hyperparameters(losses)[1:3]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in best_params.items():\n",
    "    print(f'{key} - {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    lambda_ =\n",
    "    gamma =\n",
    "    w, loss = train(model, y_tr_set, clean_train, initial_w, max_iters, gamma, lambda_)\n",
    "    train_predictions = np.where(np.dot(clean_train,w) < mapping_threshold, -1, 1)\n",
    "    test_predictions = np.where(np.dot(clean_test, w) < mapping_threshold, -1, 1)\n",
    "    train_err = y_tr_set - train_predictions\n",
    "    test_err = y_te_set - test_predictions\n",
    "    print('')\n",
    "    print(f'=========== {model} ===========')\n",
    "    print(f'train error = {train_err} - test error = {test_err}')\n",
    "    train_metrics = calculate_metrics(y_tr_set, train_predictions)\n",
    "    test_metrics = calculate_metrics(y_te_set, test_predictions)\n",
    "    print('=========== TRAIN metrics ===========')\n",
    "    prettyprint(train_metrics)\n",
    "    print('=========== TEST metrics ===========')\n",
    "    prettyprint(test_metrics)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat all steps for total train and make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "#some parameters might need to be taken from hyperparameters\n",
    "\n",
    "tot_clean_train = clean(x_tr, nan_threshold, \n",
    "                    remove_const, const_thresholds, \n",
    "                    PCA, n_components, pca_thresholds, \n",
    "                    max_unique_values, correlation_threshold)\n",
    "\n",
    "tot_clean_test = clean(x_tr, nan_threshold, \n",
    "                    remove_const, const_thresholds, \n",
    "                    PCA, n_components, pca_thresholds, \n",
    "                    max_unique_values, correlation_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on train and predict + submit prediction\n",
    "for model in models:\n",
    "    lambda_ =\n",
    "    gamma =\n",
    "    w, loss = train(model, y_tr_set, clean_train, initial_w, max_iters, gamma, lambda_)\n",
    "    y_pred = np.where(np.dot(clean_train,w) < mapping_threshold, -1, 1)\n",
    "    create_csv_submission([i for i in range(len(y_pred))], y_pred, model.replace( , '_') + '.submission')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
